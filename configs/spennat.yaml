dataset: data/fra-eng/fra.txt
spacy_model: en_trf_bertbaseuncased_lg
vocab_count_threshold: 1

val_interval: 10
device: -1
num_samples: 10000

num_epochs: 500
batch_size: 20
flip_prob: null

entropy_coef: 0.1

clip_grad_norm: null
clip_grad: null

# optimizer: adadelta(lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)
# optimizer: adagrad(lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)
# optimizer: adam(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)
optimizer: adamw(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
# optimizer: adamax(lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
# optimizer: asgd(lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)
# optimizer: lbfgs(lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None)
# optimizer: rmsprop(lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
# optimizer: rmsprop(lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
# optimizer: rprop(lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))
# optimizer: sgd(lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)

inference:
  use_sqrt_decay: true
  iterations: 10000
  learning_rate: 1.0
  eps: 0.0001
  region_eps: null

d_model: &d_model 512

feature_network:
  d_model: *d_model
  dropout: 0.5
  num_layers: 2

global_network:
  d_model: *d_model
  nhead: 8
  dim_feedforward: 2048
  activation: relu
  dropout: 0.1
  num_layers: 1
